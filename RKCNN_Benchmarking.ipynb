{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d5c6659-a0bf-4ba9-ae5e-95633be98a32",
   "metadata": {},
   "source": [
    "## **RKCNN (Random K Conditional Nearest Neighbor)**\n",
    "**Implementing this new variant of KNN and benchmarking against KNN and CKNN with high-dimensional data**\n",
    "\n",
    "**What is RKCNN?** Random Conditional K-Nearest Neighbors (**RKCNN**) is a classification algorithm that **enhances traditional KNN** by introducing conditional probabilities and random feature subset selection. Unlike standard KNN, which relies on global nearest neighbors, RKCNN calculates conditional probabilities based on neighbors within randomly selected feature subsets, **weighted by their separation scores**. This approach allows RKCNN to **handle high-dimensional, noisy data** by focusing on informative feature combinations, building upon both KNN's nearest neighbor concept and KCNN's conditional probability estimation.\n",
    "\n",
    "This new variant of KNN and KCNN was first shared by researchers Jiaxuan Lu and Hyukjun Gweon in their paper [Random Conditional K-Nearest Neighbors (RKCNN) Paper](https://peerj.com/articles/cs-2497/) on January 24, 2025. In the following notebook, I will be **recreating my own RKCNN from the math and pseudo code** provided in their paper (which is not exactly the same as the python code for the RKCNN algorithm that the researchers provided) and I will benchmark it to other algorithms such as KNN, KCNN, XGBoost, and BERT on a high-dimensional, noisy data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ced7fc-0930-498a-99ae-2867266f3c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from joblib import Parallel, delayed\n",
    "from itertools import product\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, adjusted_rand_score, classification_report, f1_score, matthews_corrcoef, pairwise_distances, precision_score, recall_score, silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from tqdm_joblib import tqdm_joblib\n",
    "from transformers import AdamW, BertForSequenceClassification, BertTokenizer\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1858ea4f-1fb7-4c33-aca8-3cce879a4850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RKCNN and KCNN\n",
    "class RandomKConditionalNeighbors:\n",
    "    def __init__(self, n_neighbors=5, h=10, r=5, m=0.5, smoothing=1, **kwargs):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.h = h\n",
    "        self.r = r\n",
    "        self.m = m\n",
    "        self.smoothing = smoothing\n",
    "        self.models = []\n",
    "        self.feature_subsets = []\n",
    "        self.separation_scores = []\n",
    "        self.weights = []\n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "    def calculate_their_separation_score(self, X, y, feature_subset):\n",
    "        \"\"\"\n",
    "        Calculates the separation score using the method described in the paper.\n",
    "        \"\"\"\n",
    "        if isinstance(y, pd.DataFrame):\n",
    "            y = y.iloc[:, 0]\n",
    "\n",
    "        centers = [np.mean(X[y == i], axis=0) for i in set(y)]\n",
    "        overall_mean = np.mean(X, axis=0)\n",
    "\n",
    "        A = np.array([(center - overall_mean) ** 2 for center in centers])\n",
    "\n",
    "        B = []\n",
    "        Nc = []\n",
    "        for i in set(y):\n",
    "            class_data = X[y == i]\n",
    "            Nc.extend([1 / class_data.shape[0]] * class_data.shape[0])\n",
    "            for _, row in class_data.iterrows():\n",
    "                B.append((row - centers[list(set(y)).index(i)]) ** 2)\n",
    "        B = np.array(B)\n",
    "        Nc = np.array(Nc)\n",
    "\n",
    "        a = np.array([1 if feature in feature_subset else 0 for feature in X.columns])\n",
    "\n",
    "        aA = np.sqrt(np.dot(A, a)).sum()\n",
    "        aB = np.sqrt(np.dot(B, a) * Nc).sum()\n",
    "\n",
    "        return aA / aB if aB != 0 else 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the RKCNN model by generating and training KCNN models on feature subsets.\n",
    "        \"\"\"\n",
    "        if self.r > self.h:\n",
    "            raise ValueError(f\"Invalid parameters: r ({self.r}) cannot be larger than h ({self.h}).\")\n",
    "\n",
    "        self.models = []\n",
    "        self.feature_subsets = []\n",
    "        self.separation_scores = []\n",
    "\n",
    "        # Ensure y is a pandas Series for compatibility\n",
    "        y = pd.Series(y) if not isinstance(y, pd.Series) else y\n",
    "\n",
    "        # Fit the LabelEncoder with the target labels\n",
    "        self.label_encoder.fit(y)\n",
    "\n",
    "        for _ in range(self.h):\n",
    "            num_features = int(len(X.columns) * self.m) if isinstance(self.m, float) else self.m;\n",
    "            subset = X.sample(n=num_features, axis=1)\n",
    "            self.feature_subsets.append(subset.columns)\n",
    "\n",
    "            score = self.calculate_their_separation_score(subset, y, subset.columns)\n",
    "            self.separation_scores.append(score)\n",
    "\n",
    "        subset_scores = pd.DataFrame({\n",
    "            \"subset\": self.feature_subsets,\n",
    "            \"score\": self.separation_scores,\n",
    "        }).sort_values(by=\"score\", ascending=False).iloc[:self.r]\n",
    "\n",
    "        self.feature_subsets = subset_scores[\"subset\"].tolist()\n",
    "        self.separation_scores = subset_scores[\"score\"].tolist()\n",
    "\n",
    "        for subset in self.feature_subsets:\n",
    "            subset_X = X.loc[:, subset]\n",
    "            model = KConditionalNeighbors(n_neighbors=self.n_neighbors, smoothing=self.smoothing)\n",
    "            model.fit(subset_X, y)\n",
    "            self.models.append(model)\n",
    "\n",
    "        total_score = sum(self.separation_scores)\n",
    "        self.weights = [score / total_score for score in self.separation_scores]\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict class probabilities by aggregating predictions from subset-specific models.\n",
    "        \"\"\"\n",
    "        final_probabilities = pd.DataFrame(0, index=X.index, columns=self.label_encoder.classes_)\n",
    "\n",
    "        for model, weight, subset in zip(self.models, self.weights, self.feature_subsets):\n",
    "            subset_X = X.loc[:, subset]\n",
    "            subset_probabilities = pd.DataFrame(\n",
    "                model.predict_proba(subset_X),\n",
    "                index=X.index,\n",
    "                columns=self.label_encoder.classes_,\n",
    "            )\n",
    "            final_probabilities += subset_probabilities * weight\n",
    "\n",
    "        return final_probabilities.div(final_probabilities.sum(axis=1), axis=0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels based on aggregated probabilities.\n",
    "        \"\"\"\n",
    "        probabilities = self.predict_proba(X)\n",
    "        return probabilities.idxmax(axis=1)\n",
    "\n",
    "class KConditionalNeighbors(KNeighborsClassifier):\n",
    "    def __init__(self, n_neighbors=5, smoothing=1, metric='minkowski', **kwargs):\n",
    "        super().__init__(n_neighbors=n_neighbors, metric=metric, **kwargs)\n",
    "        self.smoothing = smoothing\n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.label_encoder.fit(y)\n",
    "        y_encoded = self.label_encoder.transform(y)\n",
    "        super().fit(X.to_numpy(), y_encoded)\n",
    "        self._y = np.array(y)\n",
    "        self._y_encoded = y_encoded\n",
    "        self.classes_ = np.unique(y)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        probabilities = np.zeros((X.shape[0], len(self.classes_)))\n",
    "        skipped_classes = set()\n",
    "\n",
    "        for class_index, class_label in enumerate(self.classes_):\n",
    "            encoded_class_label = self.label_encoder.transform([class_label])[0]\n",
    "            class_knn = KNeighborsClassifier(n_neighbors=self.n_neighbors, metric=self.metric)\n",
    "            class_data = self._fit_X[self._y_encoded == encoded_class_label]\n",
    "\n",
    "            if class_data.shape[0] == 0:\n",
    "                skipped_classes.add(class_label)\n",
    "                continue\n",
    "\n",
    "            class_knn.fit(class_data, self._y_encoded[self._y_encoded == encoded_class_label])\n",
    "            class_distances, _ = class_knn.kneighbors(X.to_numpy())\n",
    "\n",
    "            class_prob_k = np.zeros((X.shape[0], self.n_neighbors))\n",
    "            for k_index in range(self.n_neighbors):\n",
    "                kth_distances = class_distances[:, k_index]\n",
    "                denom = np.sum(self.n_neighbors / (kth_distances + self.smoothing) ** (self.p / self.n_features_in_))\n",
    "                if denom > 0:\n",
    "                    class_prob_k[:, k_index] = (self.n_neighbors / (kth_distances + self.smoothing) ** (self.p / self.n_features_in_)) / denom\n",
    "                else:\n",
    "                    class_prob_k[:, k_index] = 1 / self.n_neighbors\n",
    "\n",
    "            probabilities[:, class_index] = np.mean(class_prob_k, axis=1)\n",
    "\n",
    "        if skipped_classes:\n",
    "            print(f\"WARNING: The following classes were skipped due to lack of samples: {sorted(skipped_classes)}\")\n",
    "\n",
    "        row_sums = probabilities.sum(axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1\n",
    "        probabilities = probabilities / row_sums\n",
    "        return probabilities\n",
    "\n",
    "    def predict(self, X):\n",
    "        probabilities = self.predict_proba(X)\n",
    "        predictions = self.classes_[np.argmax(probabilities, axis=1)]\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d57c2ba-635e-461e-9d74-d5b8c6570d96",
   "metadata": {},
   "source": [
    "## **How to Calculate RKCNN**\n",
    "\n",
    "### **The First Step: Implementing KCNN:** \n",
    "Whereas traditional KNN relies on global nearest neighbors to classify a new query vector, K Conditional Nearest Neighbors (KCNN) instead **calculates conditional probabilities** of a query vector belonging to a certain class based on patterns and distributions of its nearest neighbors.\n",
    "\n",
    "The conditional probabilities can be calculated by the following formula...\n",
    "\n",
    "$$\\hat{P}_{j}(Y=c|x) = \\frac{||x',x'_{k|c}||^{-1/2}}{\\sum_{l=1}^{L} ||x',x'_{k|l}||^{-1/2}}$$\n",
    "\n",
    "### **Implementing RKCNN:** \n",
    "RKCNN expands KCNN by training an ensemble of `h` KCNN models on random feature subsets, selecting the top `r` models based on their separation score `s`, and aggregating their predictions, where each base KCNN considers `k` nearest neighbors.\n",
    "\n",
    "**Separation Score (`s`)**\n",
    "The separation score evaluates the discriminative power of a selected feature subset by balancing between-class and within-class variance. It is defined as:\n",
    "\n",
    "$$S = \\frac{BV}{WV} = \\frac{\\left[ \\sum_{c=1}^{L} \\|\\bar{x}_c - \\bar{x}'\\|^2 \\frac{1}{L-1} \\right]}{\\left[ \\sum_{c=1}^{L} \\sum_{i_c=1}^{N_c} \\|x'_{i_c} - \\bar{x}'_c\\|^2 \\frac{1}{(N_c-1)L} \\right]}$$\n",
    "\n",
    "---\n",
    "\n",
    "**Between-Class and Within-Class Variance (BV and WV)**\n",
    "\n",
    "$$BV = \\frac{\\sqrt{\\sum_{c=1}^{L} b_c^T z_j}}{L-1}$$\n",
    "\n",
    "$$WV = \\frac{\\sqrt{\\sum_{i=1}^{N} w_i^T z_j}}{L}$$\n",
    "\n",
    "---\n",
    "\n",
    "**Classifier Weights** $w^{(j)}$\n",
    "The weight of a classifier for a given feature subset is derived from its separation score:\n",
    "\n",
    "$$w^{(j)} = \\frac{S^{(j)}}{\\sum_{l=1}^{r} S^{(l)}}$$\n",
    "\n",
    "---\n",
    "\n",
    "**Aggregated Probability** $(\\hat{P}(Y=c|x))$\n",
    "To classify a query vector, we aggregate probabilities from the top $r$ subset-specific classifiers, weighted by their separation scores:\n",
    "\n",
    "$$\\hat{P}(Y=c|x) = \\sum_{j=1}^{r} \\left[ \\hat{P}^{(j)}(Y=c|x) \\cdot w^{(j)} \\right]$$\n",
    "\n",
    "---\n",
    "\n",
    "**Classification Rule** $(\\hat{Y})$\n",
    "The final predicted class label is chosen as the one with the maximum aggregated probability:\n",
    "\n",
    "$$\\hat{Y} = \\underset{c}{\\operatorname{argmax}} \\hat{P}(Y=c|x)$$\n",
    "\n",
    "---\n",
    "\n",
    "<font color=\"#7a7e81\"><font size=\"-1\"><i>Equation credit: Lu and Gweon (2025)</i></font></font>\n",
    "\n",
    "These formulas not only provide a mathematical foundation for RKCNN but also drive its ability to identify highly separable feature subsets, ensuring robust classification in high-dimensional, noisy datasets.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8235679f-db58-478d-bfb6-ff65d489f318",
   "metadata": {},
   "source": [
    "### **Benchmarking RKCNN**\n",
    "For benchmarking purposes I chose the well-known 20 Newsgroups dataset, which I preprocessed using Sentence-BERT and LSH. If you would like to experiment with this dataset yourself, I have provided the CSV file in the repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea464f3-0d05-4ff9-bbe5-8f653179e4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Functions\n",
    "def evaluate_classification(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision_micro = precision_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall_micro = recall_score(y_true, y_pred, average='micro')\n",
    "    recall_macro = recall_score(y_true, y_pred, average='macro')\n",
    "    recall_weighted = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    return {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision (micro)': precision_micro,\n",
    "        'Precision (macro)': precision_macro,\n",
    "        'Precision (weighted)': precision_weighted,\n",
    "        'Recall (micro)': recall_micro,\n",
    "        'Recall (macro)': recall_macro,\n",
    "        'Recall (weighted)': recall_weighted,\n",
    "        'F1-score (micro)': f1_micro,\n",
    "        'F1-score (macro)': f1_macro,\n",
    "        'F1-score (weighted)': f1_weighted,\n",
    "        'MCC': mcc\n",
    "    }\n",
    "\n",
    "def evaluate_clustering(X, labels_true, labels_pred):\n",
    "    ari = adjusted_rand_score(labels_true, labels_pred)\n",
    "    silhouette = silhouette_score(X, labels_pred)\n",
    "    return {\n",
    "        'ARI': ari,\n",
    "        'Silhouette Score': silhouette\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d43b78b-1b3a-45fb-ad5f-20ff3e1f3715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the 20 Newsgroup dataset\n",
    "# Load the CSV file\n",
    "loaded_data = pd.read_csv('/Users/Rittersport/Desktop/AI_Machine_Learning/Portfolio_Projects/RKCNN/20news_with_embeddings_and_lsh.csv')\n",
    "\n",
    "# Function to convert string representation of embeddings to NumPy array\n",
    "def string_to_array(string_representation):\n",
    "    string_representation = string_representation.strip(\"[]\")\n",
    "    # Replace newlines and multiple spaces with a single space, then split\n",
    "    string_representation = ' '.join(string_representation.split())\n",
    "    string_representation = string_representation.split(\" \")\n",
    "    array = np.array([float(val) for val in string_representation])\n",
    "    return array\n",
    "\n",
    "# Convert string embeddings to NumPy arrays\n",
    "loaded_data['sentence_bert_embeddings'] = loaded_data['sentence_bert_embeddings'].apply(string_to_array)\n",
    "\n",
    "# Sentence-BERT embeddings as features\n",
    "X = pd.DataFrame(loaded_data['sentence_bert_embeddings'].tolist())\n",
    "\n",
    "# Target labels\n",
    "y = loaded_data['target'].values\n",
    "\n",
    "# Single train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#reset indexes.\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d87e24-5e4a-4cac-bf58-82463c905a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Models\n",
    "trained_models = {\n",
    "    'Traditional KNN (k=4, dist)': KNeighborsClassifier(n_neighbors=4, weights='distance'),\n",
    "    'KCNN (k=2)': KConditionalNeighbors(n_neighbors=2),\n",
    "    'RCKNN (k=2)': RandomKConditionalNeighbors(n_neighbors=2, h=150, r=125, m=0.50),\n",
    "    'Linear Regression': LogisticRegression(max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100),\n",
    "    'XGBoost': XGBClassifier(eval_metric='logloss', random_state=42),\n",
    "    'SVM': SVC(random_state=42),\n",
    "    'K-Means': KMeans(n_clusters=20, n_init=10, random_state=42) # 20 categories in the dataset\n",
    "}\n",
    "\n",
    "\n",
    "# Evaluation Loop with tqdm to mark progress\n",
    "results = []\n",
    "\n",
    "for model_name, model in tqdm(trained_models.items(), desc=\"Models\"):\n",
    "    if model_name == 'K-Means':\n",
    "        model.fit(X_train)  # K-Means fits on X_train only\n",
    "        labels_pred = model.predict(X_test)\n",
    "        metrics = evaluate_clustering(X_test, y_test, labels_pred)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)  # Other classifiers fit on both X_train and y_train\n",
    "        y_pred = model.predict(X_test)\n",
    "        metrics = evaluate_classification(y_test, y_pred)\n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        **metrics\n",
    "    })\n",
    "\n",
    "# Results stored in DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e57322-7538-4b71-899d-b712e27b4b62",
   "metadata": {},
   "source": [
    "### **Benchmarking Results**\n",
    "\n",
    "**Classification Models:**\n",
    "\n",
    "<div style=\"overflow-x: auto; max-width: 550px; max-height: 550px;\">\n",
    "\n",
    "| Model                       | Accuracy   | Precision   | Recall      | F1-score   |\n",
    "|-----------------------------|------------|-------------|-------------|------------|\n",
    "| Traditional KNN (k=4, dist) | 0.732095   | 0.743271    | 0.732095    | 0.734231   |\n",
    "| KCNN (k=2)                  | 0.739257   | 0.749321    | 0.739257    | 0.738471   |\n",
    "| **RCKNN (k=2)**             | **0.747215**   | **0.758503**    | **0.747215**    | **0.746383**   |\n",
    "| Linear Regression           | 0.701592   | 0.707523    | 0.701592    | 0.699589   |\n",
    "| Random Forest               | 0.653050   | 0.651614    | 0.653050    | 0.642784   |\n",
    "| XGBoost                     | 0.684615   | 0.690002    | 0.684615    | 0.683191   |\n",
    "| SVM                         | 0.720955   | 0.730386    | 0.720955    | 0.720136   |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**K-Means:** <br>\n",
    "Adjusted Rand Index (ARI): 0.3414 <br>\n",
    "Silhouette Score: 0.0630\n",
    "\n",
    "</div style>\n",
    "\n",
    "----\n",
    "\n",
    "**Notably, the Random K Conditional Nearest Neighbor (RKCNN) algorithm achieved the highest scores across all classification metrics in this benchmark.**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49cf3f1-9ed6-47cd-87b9-bcccd6498c05",
   "metadata": {},
   "source": [
    "### **RKCNN Hyperparameter Exploration**\n",
    "The decision to benchmark **RKCNN** with the above parameters of `k=2`, `h=100`, `r=50`, and `m=0.3` came after meticulously testing RKCNN, starting with a wide range of parameters and narrowing down to certain parameter combinations after pinpointing promising results. Even though the stochastic nature of RKCNN meant that every test run would produce slightly different results, this optimization process proved to be insightful, confirming some of the reasearchers' findings - that **RKCNN is ideal for noisy data sets** and performs at its best with a **smaller values of `k`** and a **small-to-moderate values for `m`**. However, after experimenting with the ratio of `h` to `r`, I found that with this dataset, RKCNN performed best with both a **smaller values for `h` and `r`** (in contrast to the researchers findings that a large r produced the best results). Ratios where `h` was between 2 to 4 times `r` were most-suited to this dataset.\n",
    "\n",
    "If you would like to explore this optimization process yourself on your own datasets, I have provided the code for the optimization functions below, which include options for both grid and random searches to evaluate RKCNN's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec76668-bfcc-4875-8e09-3f0f1cb6ac65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate a single parameter combination\n",
    "def evaluate_params(params, X_train, y_train, X_test, y_test):\n",
    "    # Extract parameters\n",
    "    k = params['k']\n",
    "    h = params['h']\n",
    "    r = params['r']\n",
    "    m = params['m']\n",
    "\n",
    "    # Check that h >= r\n",
    "    if r >= h:\n",
    "        return {\n",
    "            'k': k,\n",
    "            'h': h,\n",
    "            'r': r,\n",
    "            'm': m,\n",
    "            'Accuracy': None,\n",
    "            'Precision': None,\n",
    "            'Recall': None,\n",
    "            'F1-score': None\n",
    "        }\n",
    "\n",
    "    # Initialize the model\n",
    "    model = RandomKConditionalNeighbors(n_neighbors=k, h=h, r=r, m=m)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "    # Return results as a dictionary\n",
    "    return {\n",
    "        'k': k,\n",
    "        'h': h,\n",
    "        'r': r,\n",
    "        'm': m,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-score': f1\n",
    "    }\n",
    "\n",
    "# Function to optimize RKCNN using grid or random search\n",
    "def optimize_rkcnn(X_train, y_train, X_test, y_test, param_ranges, search_type='grid', n_iter=50, n_jobs=-1):\n",
    "    results = []\n",
    "\n",
    "    # Generate all parameter combinations based on search type\n",
    "    if search_type == 'grid':\n",
    "        param_combinations = list(product(*param_ranges.values()))\n",
    "        \n",
    "        # Filter out invalid combinations (r >= h)\n",
    "        param_combinations = [params for params in param_combinations if params[2] < params[1]]\n",
    "\n",
    "        param_dicts = [dict(zip(param_ranges.keys(), params)) for params in param_combinations]\n",
    "\n",
    "    elif search_type == 'random':\n",
    "        param_dicts = [\n",
    "            {key: random.choice(values) for key, values in param_ranges.items()}\n",
    "            for _ in range(n_iter)\n",
    "        ]\n",
    "\n",
    "        # Filter out invalid combinations (r >= h)\n",
    "        param_dicts = [params for params in param_dicts if params['r'] < params['h']]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid search_type. Use 'grid' or 'random'.\")\n",
    "\n",
    "    print(f\"Running {search_type} search on {len(param_dicts)} parameter combinations...\")\n",
    "\n",
    "    # Ensure there are valid combinations\n",
    "    if not param_dicts:\n",
    "        raise ValueError(\"No valid parameter combinations found. Check parameter ranges and filtering logic.\")\n",
    "\n",
    "    # Run parameter evaluations with parallelization and a progress bar\n",
    "    with tqdm_joblib(tqdm(desc=\"Hyperparameter Search\", total=len(param_dicts))):\n",
    "        results = Parallel(n_jobs=n_jobs)(\n",
    "            delayed(evaluate_params)(param_dict, X_train, y_train, X_test, y_test)\n",
    "            for param_dict in param_dicts\n",
    "        )\n",
    "\n",
    "    # Convert results to a DataFrame for easier analysis\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Identify the best parameters based on accuracy\n",
    "    best_row = results_df.loc[results_df['Accuracy'].idxmax()]\n",
    "    best_params = best_row.drop(['Accuracy', 'F1-score', 'Precision', 'Recall']).to_dict()\n",
    "    best_accuracy = best_row['Accuracy']\n",
    "\n",
    "    return {'Best Parameters': best_params, 'Best Accuracy': best_accuracy}, results_df\n",
    "\n",
    "# Example Parameter Ranges\n",
    "param_ranges = {\n",
    "    'k': [4, 6, 8, 10, 12],\n",
    "    'h': [100, 150, 200, 250],\n",
    "    'r': [50, 75, 100, 125],\n",
    "    'm': [0.3, 0.4, 0.5, 0.7]\n",
    "}\n",
    "\n",
    "# Run Optimization for 20News Dataset\n",
    "print(\"Running optimization for RKCNN V2 on 20News dataset\")\n",
    "\n",
    "best_result, results_df = optimize_rkcnn(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    param_ranges, search_type='grid', n_jobs=-1\n",
    ")\n",
    "\n",
    "# Display the best result\n",
    "print(f\"Best Parameters: {best_result['Best Parameters']}\")\n",
    "print(f\"Best Accuracy: {best_result['Best Accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496a6b08-0dad-4648-be45-e2f8d780402d",
   "metadata": {},
   "source": [
    "**Results from RKCNN optimization** <br>\n",
    "Sorted by accuracy, scroll for more results:\n",
    "\n",
    "<div style=\"overflow-x: auto; max-width: 550px; max-height: 550px;\">\n",
    "    \n",
    "|   k |   h |   r |   m |   Accuracy |   Precision |   Recall |   F1-Score |\n",
    "|----:|----:|----:|----:|-----------:|------------:|---------:|-----------:|\n",
    "|   2 | 100 |  50 | 0.3 |   0.747215 |    0.758503 | 0.747215 |   0.746383 |\n",
    "|   2 | 100 |  35 | 0.3 |   0.746684 |    0.757659 | 0.746684 |   0.745729 |\n",
    "|   2 | 100 |  25 | 0.3 |   0.746419 |    0.756868 | 0.746419 |   0.744948 |\n",
    "|   2 | 100 |  50 | 0.3 |   0.746154 |    0.756477 | 0.746154 |   0.7449   |\n",
    "|   2 | 150 |  75 | 0.3 |   0.746154 |    0.756611 | 0.746154 |   0.744946 |\n",
    "|   2 | 100 |  50 | 0.5 |   0.745889 |    0.756114 | 0.745889 |   0.74502  |\n",
    "|   2 | 100 |  50 | 0.2 |   0.745889 |    0.756709 | 0.745889 |   0.74427  |\n",
    "|   2 | 150 |  75 | 0.3 |   0.745889 |    0.756835 | 0.745889 |   0.745066 |\n",
    "|   3 | 150 |  75 | 0.3 |   0.745623 |    0.755746 | 0.745623 |   0.743657 |\n",
    "|   2 | 100 |  25 | 0.3 |   0.745623 |    0.757293 | 0.745623 |   0.744948 |\n",
    "|   3 | 140 |  70 | 0.2 |   0.745358 |    0.756737 | 0.745358 |   0.743021 |\n",
    "|   3 | 150 |  50 | 0.3 |   0.745358 |    0.756084 | 0.745358 |   0.743403 |\n",
    "|   3 | 150 |  50 | 0.3 |   0.745093 |    0.755956 | 0.745093 |   0.743166 |\n",
    "|   3 | 100 |  50 | 0.3 |   0.745093 |    0.755956 | 0.745093 |   0.743166 |\n",
    "|   2 | 150 |  75 | 0.5 |   0.745093 |    0.75512  | 0.745093 |   0.744172 |\n",
    "|   3 | 175 |  85 | 0.3 |   0.745093 |    0.755902 | 0.745093 |   0.743071 |\n",
    "|   2 | 100 |  25 | 0.2 |   0.745093 |    0.756314 | 0.745093 |   0.743679 |\n",
    "|   3 | 160 |  80 | 0.3 |   0.744828 |    0.755968 | 0.744828 |   0.742948 |\n",
    "|   3 | 160 |  80 | 0.2 |   0.744297 |    0.755666 | 0.744297 |   0.741891 |\n",
    "|   3 | 150 |  35 | 0.3 |   0.744297 |    0.755468 | 0.744297 |   0.742358 |\n",
    "|   4 | 250 | 125 | 0.3 |   0.743767 |    0.75612  | 0.743767 |   0.741606 |\n",
    "|   4 | 250 |  75 | 0.2 |   0.743767 |    0.757215 | 0.743767 |   0.741403 |\n",
    "|   2 | 100 |  25 | 0.5 |   0.743767 |    0.75351  | 0.743767 |   0.742574 |\n",
    "|   3 | 175 |  85 | 0.3 |   0.743767 |    0.75541  | 0.743767 |   0.741898 |\n",
    "|   4 | 100 |  75 | 0.3 |   0.743501 |    0.755658 | 0.743501 |   0.741256 |\n",
    "|   3 | 125 |  60 | 0.3 |   0.743501 |    0.754207 | 0.743501 |   0.74143  |\n",
    "|   4 | 100 |  75 | 0.2 |   0.743236 |    0.755747 | 0.743236 |   0.740601 |\n",
    "|   3 | 140 |  70 | 0.3 |   0.743236 |    0.754412 | 0.743236 |   0.741268 |\n",
    "|   4 | 100 |  50 | 0.2 |   0.742971 |    0.755003 | 0.742971 |   0.740233 |\n",
    "|   3 | 150 |  75 | 0.5 |   0.742971 |    0.752542 | 0.742971 |   0.741105 |\n",
    "|   4 | 250 |  75 | 0.5 |   0.742971 |    0.754899 | 0.742971 |   0.740902 |\n",
    "|   2 | 100 |  10 | 0.3 |   0.742971 |    0.755171 | 0.742971 |   0.742142 |\n",
    "|   4 | 100 |  50 | 0.3 |   0.742706 |    0.755075 | 0.742706 |   0.740397 |\n",
    "|   4 | 150 |  50 | 0.3 |   0.742706 |    0.756225 | 0.742706 |   0.740779 |\n",
    "|   4 | 150 |  75 | 0.5 |   0.742706 |    0.755011 | 0.742706 |   0.740777 |\n",
    "|   3 | 100 |  50 | 0.5 |   0.742706 |    0.752218 | 0.742706 |   0.740799 |\n",
    "|   4 | 250 | 100 | 0.5 |   0.74244  |    0.754175 | 0.74244  |   0.7402   |\n",
    "|   4 | 300 | 150 | 0.3 |   0.74244  |    0.755246 | 0.74244  |   0.740107 |\n",
    "|   4 | 400 |  50 | 0.5 |   0.74244  |    0.754321 | 0.74244  |   0.74048  |\n",
    "|   3 | 150 |  25 | 0.3 |   0.74244  |    0.7543   | 0.74244  |   0.740748 |\n",
    "|   4 | 100 |  75 | 0.5 |   0.742175 |    0.75376  | 0.742175 |   0.739998 |\n",
    "|   4 | 150 |  75 | 0.3 |   0.742175 |    0.754867 | 0.742175 |   0.739978 |\n",
    "|   4 | 100 |  75 | 0.4 |   0.742175 |    0.75394  | 0.742175 |   0.739979 |\n",
    "|   4 | 300 |  75 | 0.3 |   0.742175 |    0.754994 | 0.742175 |   0.739869 |\n",
    "|   4 | 300 |  50 | 0.5 |   0.74191  |    0.753612 | 0.74191  |   0.739759 |\n",
    "|   4 | 400 |  50 | 0.3 |   0.74191  |    0.75473  | 0.74191  |   0.739521 |\n",
    "|   3 | 140 |  70 | 0.4 |   0.74191  |    0.752155 | 0.74191  |   0.739671 |\n",
    "|   4 | 150 |  50 | 0.5 |   0.741645 |    0.753983 | 0.741645 |   0.739451 |\n",
    "|   3 | 125 |  60 | 0.5 |   0.741645 |    0.751768 | 0.741645 |   0.73978  |\n",
    "|   3 | 160 |  80 | 0.4 |   0.741645 |    0.752021 | 0.741645 |   0.739468 |\n",
    "|   4 | 300 | 150 | 0.5 |   0.741379 |    0.753245 | 0.741379 |   0.739237 |\n",
    "|   4 | 100 |  50 | 0.5 |   0.741114 |    0.752251 | 0.741114 |   0.73895  |\n",
    "|   4 | 250 |  75 | 0.4 |   0.741114 |    0.75294  | 0.741114 |   0.73874  |\n",
    "|   6 | 150 |  50 | 0.5 |   0.740849 |    0.752361 | 0.740849 |   0.738276 |\n",
    "|   4 | 300 |  50 | 0.3 |   0.740849 |    0.753181 | 0.740849 |   0.738704 |\n",
    "|   4 | 250 | 100 | 0.3 |   0.740584 |    0.752567 | 0.740584 |   0.737892 |\n",
    "|   4 | 300 |  75 | 0.5 |   0.740584 |    0.752032 | 0.740584 |   0.73851  |\n",
    "|   6 | 150 |  75 | 0.5 |   0.740053 |    0.752447 | 0.740053 |   0.737771 |\n",
    "|   6 | 100 |  50 | 0.5 |   0.739788 |    0.752282 | 0.739788 |   0.737244 |\n",
    "|   6 | 150 |  50 | 0.3 |   0.739788 |    0.752253 | 0.739788 |   0.736954 |\n",
    "|   3 | 175 |  85 | 0.5 |   0.739788 |    0.750352 | 0.739788 |   0.737916 |\n",
    "|   4 | 250 |  75 | 0.3 |   0.739523 |    0.751656 | 0.739523 |   0.736954 |\n",
    "|   6 | 150 |  75 | 0.3 |   0.739257 |    0.752669 | 0.739257 |   0.736242 |\n",
    "|   6 | 100 |  75 | 0.5 |   0.738992 |    0.750987 | 0.738992 |   0.736552 |\n",
    "|   6 | 100 |  75 | 0.3 |   0.738727 |    0.751527 | 0.738727 |   0.736074 |\n",
    "|   6 | 100 |  50 | 0.3 |   0.738196 |    0.750277 | 0.738196 |   0.735282 |\n",
    "|   8 | 100 |  75 | 0.3 |   0.738196 |    0.75082  | 0.738196 |   0.734874 |\n",
    "|   8 | 100 |  75 | 0.5 |   0.736605 |    0.748921 | 0.736605 |   0.733519 |\n",
    "|   8 | 100 |  50 | 0.3 |   0.736074 |    0.748736 | 0.736074 |   0.73235  |\n",
    "|   8 | 100 |  50 | 0.5 |   0.736074 |    0.749654 | 0.736074 |   0.733009 |\n",
    "|   8 | 150 |  75 | 0.3 |   0.735544 |    0.748839 | 0.735544 |   0.732063 |\n",
    "|   8 | 150 |  75 | 0.5 |   0.735544 |    0.749224 | 0.735544 |   0.732265 |\n",
    "|  10 | 250 | 100 | 0.7 |   0.727851 |    0.742587 | 0.727851 |   0.723952 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33704a62-4baf-4bab-b09f-113fd8382c5c",
   "metadata": {},
   "source": [
    "## **Benchmarking to BERT**\n",
    "BERT base-uncased, a powerful transformer model, achieved an overall **accuracy of 74.67%** after 5 epochs.  Notably, **our RKCNN model achieved a slightly higher overall accuracy of 74.72%** on the same dataset.\n",
    "\n",
    "The following code can be used to run BERT for your own comparisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b501366c-b5bc-4d9d-a11f-aab9224bc8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Initialize the pre-trained BERT model\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=20)\n",
    "model.to(device)  # Move the model to the GPU if available\n",
    "\n",
    "# Tokenize the data\n",
    "texts = loaded_data['cleaned_text'].tolist() # Replace 'loaded_data' with your DataFrame\n",
    "labels = loaded_data['target'].tolist() # Replace 'loaded_data' with your DataFrame\n",
    "\n",
    "encodings = tokenizer(texts, truncation=True, padding=True, return_tensors='pt')\n",
    "input_ids = encodings['input_ids'].to(device)  # Move input_ids to the GPU if available\n",
    "attention_mask = encodings['attention_mask'].to(device)  # Move attention_mask to the GPU if available\n",
    "labels = torch.tensor(labels).to(device)  # Move labels to the GPU if available\n",
    "\n",
    "# Split the data\n",
    "train_inputs, test_inputs, train_labels, test_labels, train_masks, test_masks = train_test_split(\n",
    "    input_ids, labels, attention_mask, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create PyTorch DataLoaders to efficiently load data in batches\n",
    "batch_size = 16\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "# Fine-tune the BERT Model\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "epochs = 5 # Change to your desired number of epochs\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)  # Move batch to GPU if available\n",
    "        attention_mask = attention_mask.to(device)  # Move batch to GPU if available\n",
    "        labels = labels.to(device)  # Move batch to GPU if available\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(f'Average training loss: {avg_train_loss:.4f}')\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    total_eval_loss = 0\n",
    "    for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)  # Move batch to GPU if available\n",
    "        attention_mask = attention_mask.to(device)  # Move batch to GPU if available\n",
    "        labels = labels.to(device)  # Move batch to GPU if available\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_eval_loss += loss.item()\n",
    "        logits = outputs.logits\n",
    "        predictions.extend(torch.argmax(logits, dim=-1).tolist())\n",
    "        true_labels.extend(labels.tolist())\n",
    "\n",
    "    avg_eval_loss = total_eval_loss / len(test_dataloader)\n",
    "    print(f'Average evaluation loss: {avg_eval_loss:.4f}')\n",
    "    print(f'Accuracy: {accuracy_score(true_labels, predictions):.4f}')\n",
    "    print(classification_report(true_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25037401-d7a0-4164-8a01-ec0271d6564e",
   "metadata": {},
   "source": [
    "**BERT Results:**\n",
    "\n",
    "**Overall Accuracy:** 0.7467\n",
    "\n",
    "<div style=\"overflow-x: auto; max-width: 550px; max-height: 550px;\">\n",
    "\n",
    "**Overall Results:**\n",
    "|             |   precision |    recall |  f1-score |   support |\n",
    "|------------:|------------:|----------:|-----------:|----------:|\n",
    "| macro avg   |      0.75 |    0.74 |     0.74 |      3770 |\n",
    "| weighted avg|      0.76 |    0.75 |     0.75 |      3770 |\n",
    "\n",
    "**Results Per-Class (20 Classes in Total):**\n",
    "|   category |   precision |   recall |   f1-score |   support |\n",
    "|-----------:|------------:|---------:|-----------:|----------:|\n",
    "|          0 |       0.55 |     0.58 |       0.57 |       151 |\n",
    "|          1 |       0.65 |     0.78 |       0.71 |       202 |\n",
    "|          2 |       0.72 |     0.71 |       0.71 |       195 |\n",
    "|          3 |       0.61 |     0.72 |       0.66 |       183 |\n",
    "|          4 |       0.80 |     0.73 |       0.76 |       205 |\n",
    "|          5 |       0.86 |     0.88 |       0.87 |       215 |\n",
    "|          6 |       0.83 |     0.76 |       0.79 |       193 |\n",
    "|          7 |       0.54 |     0.74 |       0.62 |       196 |\n",
    "|          8 |       0.68 |     0.79 |       0.73 |       168 |\n",
    "|          9 |       0.97 |     0.85 |       0.91 |       211 |\n",
    "|         10 |       0.95 |     0.88 |       0.91 |       198 |\n",
    "|         11 |       0.88 |     0.74 |       0.80 |       201 |\n",
    "|         12 |       0.73 |     0.70 |       0.72 |       202 |\n",
    "|         13 |       0.92 |     0.85 |       0.88 |       194 |\n",
    "|         14 |       0.82 |     0.78 |       0.80 |       189 |\n",
    "|         15 |       0.78 |     0.81 |       0.79 |       202 |\n",
    "|         16 |       0.66 |     0.80 |       0.72 |       188 |\n",
    "|         17 |       0.84 |     0.80 |       0.82 |       182 |\n",
    "|         18 |       0.81 |     0.45 |       0.58 |       159 |\n",
    "|         19 |       0.42 |     0.38 |       0.40 |       136 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72258b1-b56c-4f15-866b-6333984f8f88",
   "metadata": {},
   "source": [
    "### **RKCNN Results Per-Class**\n",
    "In order to provide the best direct comparision to BERT, I created a function to evaluate RKCNN's performance per-class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6337b0a2-0b22-4ec2-af19-0126f930e310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification_per_class(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluates a classification model and prints per-class metrics in a tabular format.\n",
    "\n",
    "    Args:\n",
    "        model: The trained classification model (RKCNN model).\n",
    "        X_test: The test data.\n",
    "        y_test: The true labels for the test data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing per-class precision, recall, F1-score, and support.\n",
    "    \"\"\"\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Generate the classification report as a dictionary\n",
    "    report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    # Prepare a DataFrame for the per-class results\n",
    "    per_class_results = pd.DataFrame(report_dict).transpose()\n",
    "    per_class_results = per_class_results.reset_index()\n",
    "    \n",
    "    # Filter only the classes and drop the overall metrics\n",
    "    per_class_results = per_class_results.loc[per_class_results['index'].str.isdigit()]\n",
    "    per_class_results = per_class_results.rename(columns={\n",
    "        'index': 'Category',\n",
    "        'precision': 'Precision',\n",
    "        'recall': 'Recall',\n",
    "        'f1-score': 'F1-score',\n",
    "        'support': 'Support'\n",
    "    }).astype({'Category': int, 'Support': int})\n",
    "    \n",
    "    # Print overall accuracy\n",
    "    overall_accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"**Overall Accuracy:** {overall_accuracy:.2f}\\n\")\n",
    "    \n",
    "    # Print the DataFrame in tabular format\n",
    "    print(\"**Per-Class Results:**\")\n",
    "    print(per_class_results.to_string(index=False, float_format=\"%.2f\"))\n",
    "\n",
    "    return per_class_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d4ccd4-9fca-4981-b27b-550f4e24ef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "RKCNN_per_cat = RandomKConditionalNeighbors(n_neighbors=2, h=100, r=50, m=0.3)\n",
    "\n",
    "# Train the model\n",
    "RKCNN_per_cat.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the trained model per category\n",
    "per_class_results_df = evaluate_classification_per_class(RKCNN_per_cat, X_test, y_test)\n",
    "\n",
    "# Print the results\n",
    "print(per_class_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059b3380-9ea2-4a2e-bef7-185eb385d048",
   "metadata": {},
   "source": [
    "### **RKCNN Results Per Class:**\n",
    "\n",
    "<div style=\"overflow-x: auto; max-width: 550px; max-height: 700px;\">\n",
    "\n",
    "**Overall Accuracy:** 0.75\n",
    "\n",
    "| Category | Precision | Recall | F1-score | Support |\n",
    "|---------:|----------:|-------:|---------:|--------:|\n",
    "|        0 |     0.70 |   0.50 |     0.58 |     151 |\n",
    "|        1 |     0.80 |   0.71 |     0.75 |     202 |\n",
    "|        2 |     0.74 |   0.64 |     0.69 |     195 |\n",
    "|        3 |     0.57 |   0.73 |     0.64 |     183 |\n",
    "|        4 |     0.78 |   0.65 |     0.71 |     205 |\n",
    "|        5 |     0.81 |   0.84 |     0.82 |     215 |\n",
    "|        6 |     0.76 |   0.75 |     0.76 |     193 |\n",
    "|        7 |     0.84 |   0.76 |     0.79 |     196 |\n",
    "|        8 |     0.80 |   0.75 |     0.78 |     168 |\n",
    "|        9 |     0.86 |   0.83 |     0.85 |     211 |\n",
    "|       10 |     0.55 |   0.93 |     0.69 |     198 |\n",
    "|       11 |     0.84 |   0.79 |     0.81 |     201 |\n",
    "|       12 |     0.79 |   0.67 |     0.72 |     202 |\n",
    "|       13 |     0.86 |   0.89 |     0.88 |     194 |\n",
    "|       14 |     0.86 |   0.79 |     0.82 |     189 |\n",
    "|       15 |     0.68 |   0.88 |     0.77 |     202 |\n",
    "|       16 |     0.72 |   0.71 |     0.72 |     188 |\n",
    "|       17 |     0.77 |   0.83 |     0.80 |     182 |\n",
    "|       18 |     0.68 |   0.65 |     0.66 |     159 |\n",
    "|       19 |     0.63 |   0.42 |     0.50 |     136 |\n",
    "\n",
    "</div style>\n",
    "\n",
    "\n",
    "Notably, RKCNN demonstrated superior performance on the potentially challenging classes 0 and 19, achieving F1-scores of 0.58 and 0.50 respectively, compared to BERT's 0.57 and 0.40.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9431ef6b-b4bf-4a3f-a1f0-2ad05935777f",
   "metadata": {},
   "source": [
    "### **Additional Comparison with BERT (Excluding Classes 0 and 19)**\n",
    "\n",
    "For an additional exploration, RKCNN was benchmarked against the reported ~80% accuracy of BERT on the 20 Newsgroups dataset, with the exclusion of classes 0 and 19 (which were observed to be potentially noisy due to their miscellaneous topic content). The results were as follows:\n",
    "\n",
    "<div style=\"overflow-x: auto; max-width: 550px; max-height: 700px;\">\n",
    "\n",
    "**RKCNN Results (Excluding Class 0 and 19):** \n",
    "\n",
    "| Model     | Accuracy | Precision | Recall   | F1-score |\n",
    "|-----------|----------|-----------|----------|----------|\n",
    "| KCNN (k=2)  | 0.7753   | 0.7844    | 0.7753   | 0.7756   |\n",
    "| RCKNN (k=2) | **0.7807** | **0.7902** | **0.7807** | **0.7810** |\n",
    "\n",
    "\n",
    "**BERT Overall Accuracy (on the same subset of classes): 0.80**\n",
    "\n",
    "</div style>\n",
    "\n",
    "---\n",
    "\n",
    "Interestingly, while RKCNN showed strong performance on this subset, its accuracy of 0.7807 was slightly lower than BERT's 0.80. This suggests that the potentially noisy classes 0 and 19 might have a different impact on the two models' performance.\n",
    "\n",
    "If you would like to benchmark this yourself, you can use the same functions above by placing the following line of code after loading the CSV, updating the variable names as sorts your workflow: <br>\n",
    "`loaded_data = loaded_data[\n",
    "(loaded_data['target'] != 0) & (loaded_data['target'] != 19)`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d318dea3-63fa-4b89-ac4f-2da5e7891694",
   "metadata": {},
   "source": [
    "### **Final Thoughts**\n",
    "The Random K Conditional Nearest Neighbor (RKCNN) algorithm has demonstrated compelling performance in this benchmark, standing strong not only against traditional KNN but also exhibiting a competitive edge against powerful transformer models like BERT. Its innovative approach, leveraging separation scores and a weighted aggregation of predictions from random feature subsets, appears to be an effective strategy for mitigating bias, variance, and overfitting, particularly in high-dimensional datasets.\n",
    "\n",
    "The inherent stochasticity of RKCNN, stemming from its random feature selection, introduces an intriguing element. While the initial thought of its application in the arts to impart a more \"human\" feel is a fascinating avenue to explore, the algorithm's robustness and ability to handle complex data suggest broader applicability across various classification domains where feature importance might be nuanced or where interpretability of feature interactions is less critical than predictive power. Future work could delve deeper into understanding the specific types of datasets and problem spaces where RKCNN offers the most significant advantages."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (riftvision)",
   "language": "python",
   "name": "riftvision_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
